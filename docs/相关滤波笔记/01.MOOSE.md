---
title: 相关滤波——MOSSE算法
date: 2021-09-15 16:17:45
permalink: /note/cf/moose
categories:
  - 相关滤波笔记
tags:
  - 
---

# 相关滤波——MOSSE算法

## 论文

[论文链接](https://github.com/Shanyaliux/TrackerCF/blob/master/paper/Visual%20Object%20Tracking%20using%20Adaptive%20Correlation%20Filters.pdf)

2010年发布在CVPR的MOSSE是相关滤波被应用于目标跟踪的开篇之作。C-COT,EOC等的算法根源来自于MOSSE。CNN是处理目标跟踪的另外一个分支，但是存在由于大规模数据集少建模困难，实时性差等缺点。

![tracking](https://cdn.jsdelivr.net/gh/Shanyaliux/comment@master/mosse/tracking.jpg)

## 算法原理

### 相关

首先，要明确相关是相关，卷积是卷积，二者不能混淆。

**定义**：互相关是用来表示两个信号之间相似性的一个度量，通常通过与已知信号比较用于寻找未知信号中的特性。

**公式**：
$$
g(i,j)=\sum_{k,l}f(i+k,j+l)h(k,l)
$$

### 与卷积的区别

- 相关就是按位去点乘

::: center

<img src="https://cdn.jsdelivr.net/gh/Shanyaliux/comment@master/mosse/correlation.png" alt="correlation" style="zoom: 15%;" />

:::

- 卷积需要把卷积核顺时针旋转180°再做点乘，其数学公式如下

$$
g(i,j)=\sum_{k,l}f(i-k,j-l)h(k,l)=\sum_{k,l}f(k,l)h(i-k,j-l)
$$

::: center

<img src="https://cdn.jsdelivr.net/gh/Shanyaliux/comment@master/mosse/convolution.png" alt="convolution" style="zoom:17%;" />

:::

- 二者在物理意义上的区别是：相关可以反应两个信号之间的相似性，而卷积不可以
- 卷积可以直接通过卷积定理(时域上的卷积等于频域上的乘积)来加速运算，而相关不可以

### 相关滤波用作跟踪的理解

使用互相关操作来定位跟踪目标在当前帧上的位置。在当前帧与滤波器进行相关运算所得出的响应图中最大值出现的位置即为跟踪目标的位置。流程如下：

::: center

<img src="https://cdn.jsdelivr.net/gh/Shanyaliux/comment@master/mosse/mosse_cross_correlation.1sohqppbz8e8.png" alt="mosse_cross_correlation"  />

:::



::: tip

此时会遇到一个问题，在时域里面进行响应图的计算所需求的运算量十分的巨大，那么我们可以使用卷积定理来将其转换到频域进行计算。

:::

### 变换到频域加速运算

相关是可以用卷积来表示的，具体公式如下：
$$
f(n)\otimes h(n)=f(n)\star h(-n)
$$
::: center

其中$\otimes$为相关计算，$\star$为卷积计算

:::

利于卷积定义将上式变换到频域为：
$$
\mathcal{F}(g)=\mathcal{F}(f\star h)=\mathcal{F}(f)\bullet \mathcal{F}(h)^*
$$
简化为：
$$
G=F\bullet H^*
$$
::: center

其中$\mathcal{F}(h(-n))=H^*$, $\bullet$表示点乘，*表示复共轭。

:::

## MOSSE算法的理解

### 预处理

1. 引入log函数，解决低照度照明的情况；

   因为自然环境相对复杂，光照的亮暗程度对图像有着很大的影响，因此我们引入log函数提高对比度。

2. 引入余弦窗口，抑制边缘效应；

   因为我们的滤波器大小是和图像的大小是完全一样的，只要稍微有一点点的偏移，就会导致滤波器超出图像的范围，所以我们进行周期卷积时会对图像的边缘进行周期填补，但是同时也带来一些不好的影响，所以我们加余弦窗口有利于突出图像的中心，减少边缘效应。

::: tip

FFT卷积算法的一个问题是图像和滤波器被映射到圆环的拓扑结构上。换句话说，它将图像的左边缘连接到右边缘，将顶部连接到底部。在卷积过程中，图像在环形空间中旋转，而不是像在空间域中那样平移。人为地连接图像的边界会引入一个影响相关输出的伪影。

:::

代码：

```python
# 图像预处理
def preProcess(self, img):
    # 获取图像的尺寸
    height, width = img.shape
    # 引入log函数
    img = np.log(img + 1)
    img = (img - np.mean(img)) / (np.std(img) + 1e-5)
    # 引入余弦窗
    window = np.outer(np.hanning(height), np.hanning(width))
    img = img * window
    return img
```

### 训练

首先将滤波器模板$H^*$与图像的目标区域$F$进行相关运算，那么在目标位置处的响应值是最大的。引入高斯响应$G$，即在理想的情况下，相关运算将以目标位置为中心，形成一个高斯响应图$G$。

滤波器模板的计算公式：
$$
H^*=\frac{G}{F}
$$
但是一般情况下，我们从一张图片上得出来的模板肯定是不准确的，所以在实际的使用中，我们常常在目标位置处进行大量的随机采样，通过最小二乘法误差的平方来得到滤波器模板$H^*$。

下方的公式中，i对应第i张训练样本图像，每个训练样本都有对应的高斯输出。
$$
\min_{H^*}\sum_i{|F_i \bullet H^*-G_i|^2}
$$
通过最小化滤波器与训练样本相关输出与理想高斯输出的差，得到最佳的滤波器模板$H^*$
$$
H^*=\frac{\sum_i{G_i\bullet F_i^*}}{\sum_i{F_i\bullet F_i^*}}
$$

### 更新模板

由于目标运动过程中存在遮挡，如果每帧都以上一帧目标所在位置进行更新有可能会将模板玷污，作者引入了学习率，使之前帧对模板的影响随着时间逐渐衰减，能在一定程度上抑制遮挡带来的问题。

公式如下：
$$
H_i^*=\frac{A_i}{B_i}
$$

$$
A_i=\eta G_i\bullet F_i^*+(1-\eta)A_{i-1}
$$

$$
B_i=\eta F_i\bullet F_i^*+(1-\eta)B_{i-1}
$$

1. $\eta$表示学习率，学习率越大，表示之前的目标外观对滤波器的影响越小。
2. $A_i$表示当前帧滤波器模板分子，$A_{i-1}$表示上一帧分子，B同理。

## 代码参考

[Github](https://github.com/Shanyaliux/TrackerCF/blob/master/tracker/Mosse.py)

::: details

```python
import os
import numpy as np
import cv2

from utils import getImgList


class Mosse:
    def __init__(self, args, img_path):

        self.args = args
        self.img_path = img_path
        self.frameList = getImgList(self.img_path)
        self.frameList.sort()

    # 开始跟踪
    def start_tracking(self):
        # 获取第一帧图像
        initImg = cv2.imread(self.frameList[0])
        # 灰度处理
        initFrame = cv2.cvtColor(initImg, cv2.COLOR_BGR2GRAY)
        initFrame = initFrame.astype(np.float32)
        # 获取跟踪框
        initGt = cv2.selectROI("moose", initImg, False, False)
        initGt = np.array(initGt).astype(np.int64)
        # 获取高斯响应
        responseMap = self.getGuassResponse(initFrame, initGt)

        # 从完整图像上抠出跟踪框
        g = responseMap[initGt[1]: initGt[1] + initGt[3], initGt[0]: initGt[0]+initGt[2]]
        fi = initFrame[initGt[1]: initGt[1] + initGt[3], initGt[0]: initGt[0]+initGt[2]]
        # 快速傅里叶变换
        G = np.fft.fft2(g)

        # 预处理
        Ai, Bi = self.preTraining(fi, G)

        # 开始跟踪
        for idx in range(len(self.frameList)):
            currentFrame = cv2.imread(self.frameList[idx])
            frameGray = cv2.cvtColor(currentFrame, cv2.COLOR_BGR2GRAY)
            frameGray = frameGray.astype(np.float32)
            if idx == 0:
                Ai = self.args.lr * Ai
                Bi = self.args.lr * Bi
                pos = initGt.copy()
                clipPos = np.array([pos[0], pos[1], pos[0]+pos[2], pos[1]+pos[3]]).astype(np.int64)
            else:
                Hi = Ai / Bi    # 公式10
                fi = frameGray[clipPos[1]:clipPos[3], clipPos[0]:clipPos[2]]
                # 预处理
                fi = self.preProcess(cv2.resize(fi, (initGt[2], initGt[3])))

                Gi = Hi * np.fft.fft2(fi)
                gi = self.linearMapping(np.fft.ifft2(Gi))

                maxValue = np.max(gi)
                maxPos = np.where(gi == maxValue)
                dy = int(np.mean(maxPos[0]) - gi.shape[0] / 2)
                dx = int(np.mean(maxPos[1]) - gi.shape[1] / 2)

                # 更新坐标
                pos[0] = pos[0] + dx
                pos[1] = pos[1] + dy

                clipPos[0] = np.clip(pos[0], 0, currentFrame.shape[1])
                clipPos[1] = np.clip(pos[1], 0, currentFrame.shape[0])
                clipPos[2] = np.clip(pos[0] + pos[2], 0, currentFrame.shape[1])
                clipPos[3] = np.clip(pos[1] + pos[3], 0, currentFrame.shape[0])
                clipPos = clipPos.astype(np.int64)

                fi = frameGray[clipPos[1]:clipPos[3], clipPos[0]:clipPos[2]]
                fi = self.preProcess(cv2.resize(fi, (initGt[2], initGt[3])))

                Ai = self.args.lr * (G * np.conjugate(np.fft.fft2(fi))) + (1 - self.args.lr) * Ai
                Bi = self.args.lr * (np.fft.fft2(fi) * np.conjugate(np.fft.fft2(fi))) + (1 - self.args.lr) * Bi

            cv2.rectangle(currentFrame, (pos[0], pos[1]), (pos[0] + pos[2], pos[1] + pos[3]), (255, 0, 0), 2)
            cv2.imshow('moose', currentFrame)
            cv2.waitKey(20)

    # 高斯响应
    def getGuassResponse(self, img, gt):
        # 获取图像的高宽
        height, width = img.shape
        # 获取网格
        xx, yy = np.meshgrid(np.arange(width), np.arange(height))
        # 获取跟踪框的重点坐标
        centerX = gt[0] + 0.5 * gt[2]
        centerY = gt[1] + 0.5 * gt[3]
        # 计算距离
        distance = (np.square(xx - centerX) + np.square(yy - centerY)) / (2 * self.args.sigma)
        # 获取响应图
        response = np.exp(-distance)
        # 归一化
        response = self.linearMapping(response)
        return response

    # 归一化
    def linearMapping(self, img):
        return (img - img.min()) / (img.max() - img.min())

    # 预训练
    def preTraining(self, initFrame, G):
        # 获取跟踪框大小
        height, width = G.shape
        # 统一尺寸
        fi = cv2.resize(initFrame, (width, height))
        # 图像预处理
        fi = self.preProcess(fi)
        # 计算 Ai Bi
        Ai = G * np.conjugate(np.fft.fft2(fi))
        Bi = np.fft.fft2(initFrame) * np.conjugate(np.fft.fft2(initFrame))
        # 随机变换
        for _ in range(self.args.num_pretrain):
            if self.args.rotate:
                fi = self.preProcess(self.random_warp(initFrame))
            else:
                fi = self.preProcess(initFrame)
            Ai = Ai + G * np.conjugate(np.fft.fft2(fi))
            Bi = Bi + np.fft.fft2(fi) * np.conjugate(np.fft.fft2(fi))
        return Ai, Bi

    # 图像预处理
    def preProcess(self, img):
        # 获取图像的尺寸
        height, width = img.shape
        # 引入log函数
        img = np.log(img + 1)
        img = (img - np.mean(img)) / (np.std(img) + 1e-5)
        # 引入余弦窗
        window = np.outer(np.hanning(height), np.hanning(width))
        img = img * window
        return img

    def random_warp(self, img):
        a = -180 / 16
        b = 180 / 16
        r = a + (b - a) * np.random.uniform()
        # rotate the image...
        matrix_rot = cv2.getRotationMatrix2D((img.shape[1] / 2, img.shape[0] / 2), r, 1)
        img_rot = cv2.warpAffine(np.uint8(img * 255), matrix_rot, (img.shape[1], img.shape[0]))
        img_rot = img_rot.astype(np.float32) / 255
        return img_rot
```

::: tip

以上代码可能不全，建议前往Github下载：[地址](https://github.com/Shanyaliux/TrackerCF/blob/master/tracker/Mosse.py)

:::

## 参考文章

[CVPR2010跟踪算法MOSSE原理及代码解析](https://blog.csdn.net/qq_17783559/article/details/82254996)

[目标跟踪之MOSSE算法(C++版本配置及原理简介)](https://blog.csdn.net/qq_40199447/article/details/105938874)

