---
title: 相关滤波——CSK算法
date: 2021-09-16 16:22:48
permalink: /note/cf/csk
categories:
  - 相关滤波笔记
tags:
  -  
---

# 相关滤波——CSK算法

## 论文

[论文链接](https://github.com/Shanyaliux/TrackerCF/blob/master/paper/Exploiting%20the%20Circulant%20Structure%20of.pdf)

## 算法概要

论文的作者在其文章的摘要中提出：

They are trained online with samples collected during tracking. Unfortunately, the potentially large number of samples becomes a computational burden, which directly conflicts with real-time requirements. On the other hand, limiting the samples may sacrifice performance. 

意思是密集采样得来的样本数量过多，会带来很大的计算负担。因此CSK算法从这点入手研究，发现密集采样的样本具有循环矩阵的结构特征。

![image](https://cdn.jsdelivr.net/gh/Shanyaliux/comment@master/csk/image.66hv66n6sj40.png)

除此之外，作者还在循环矩阵的基础上引入了FFT傅里叶变换加快了算法的速度，并通过引入核技巧将原来的图像空间转换成了非线性高位空间的回归问题，增加了准确性。

## CSK算法流程

### 密集采样

和MOSSE算法的预处理过程是一样的，都是将目标窗口进行了周期拓扑，方便后续的FFT计算。

关于MOSSE的说明可以参考 [https://shanya.world/note/cf/moose/](/note/cf/moose/)

### 带有正则项的优化问题

在MOSSE算法中是将相关转换到频域中通过最小二乘法来求解相关滤波器$H^*$：
$$
\begin{align}
\min_{H^*}\sum{|F_i\bullet H^*-G_i|^2}
\end{align}
$$
CSK提出用一个线性分类器来求解相关滤波器：
$$
\min_{w,b}\sum{L(y_i,f(x_i))+\lambda ||w||^2}
$$
其实，这个公式也叫最小二乘法，学名为正则化最小二乘法（RLS），也叫岭回归。其中$w$即为MOSSE中的相关滤波器$H$。

$L$为最二乘法的损失函数：
$$
L(y_i,f(x_i))=(y_i-f(x_i))^2
$$

$$
f(x_i)=<w,x_i>+b
$$

y是理想的高斯响应，n表示样本数量，$f(x_i)$表示图像$x_i$与滤波器w在频域内的点积，< , > 表示点积与$\bullet$一样，b没有实际意义，可以省略。所以$L(y_i,f(x_i))$即为MOSSE中的$|F_i\bullet H^*-G_i|^2$。

因此，我们会发现CSK所用的公式仅仅是在后面增加了一个正则项$\lambda ||w||^2$。

加正则项的目的是为了防止求得的滤波器过拟合。通过最小二乘法求得的相关滤波器与当前帧输入的图像F的相关性是最高的，然而我们是要用求得的滤波器结果H去预测下一帧图像中目标所在的位置。因为下一帧图像不可能和当前帧的图像一模一样，所以拟合度过高反而会影响检测的准确度，所以加入正则项来减小输入图像和滤波器之间的拟合程度，使求得的滤波器H泛化能力更强。

在此基础上引入核技巧，将会是得算法的性能更进一步的提升，将输入像素空间的信息投影到高位空间中，定义一下核函数：
$$
k(x, x^,)
$$
由岭回归的知识可以得到w的隐式表达式：
$$
w=\sum_i{\alpha _i\varphi (x_i)}
$$
引入核技巧后，对于w进行详细推导：
$$
\begin{align}
f(X_i)&=<W,\varphi(X_i)>+b\\
&=<\sum^n_{j=1}{\alpha_j\varphi(X_i),\varphi(X_i)>+b}\\
&=\sum_{j=1}{\alpha_j<\varphi(X_j),\varphi(X_i)+b}\\
&=\sum^n_{j=1}{\alpha_jk(X_j,X_i)}\\
&=\sum^n_{j=1}{\alpha_jk(X_i,X_j)}
\end{align}
$$
所以，得到：
$$
f(X_i)=K_i\alpha
$$
而对于正则项：
$$
\begin{align}
\frac{1}{2}||W||^2&=\frac{1}{2}W^TW\\
&=\frac{1}{2}\sum^n_{j=1}\alpha_j(\varphi(X_j))^T·\sum^n_{j=1}\alpha_j\varphi(X_j)\\
&=\frac{1}{2}\sum^n_{j=1}\sum^n_{j=1}\alpha_j\alpha_j<\varphi(X_j),\varphi(X_j)>\\
&=\frac{1}{2}\sum^n_{j=1}\sum^n_{j=1}\alpha_j\alpha_jK_{jj}\\
&=\alpha^2_1K_{11}+\alpha^2_1K_{11}+……\alpha^2_nK_{nn}\\
&=\alpha^TK\alpha
\end{align}
$$
原优化问题可以写成如下形式：
$$
\min\sum^m_{i=1}{(Y_i-K_i\alpha)^2+\lambda\alpha^TK\alpha}
$$

$$
\min[(Y-K\alpha)^T(Y-K\alpha)+\lambda\alpha^TK\alpha]
$$

求导得到$\alpha$：
$$
\frac{\partial}{\partial\alpha}\min[(Y-K\alpha)^T(Y-K\alpha)+\lambda\alpha^TK\alpha]
$$

$$
=\min(Y-K\alpha)^T(-K)+\lambda\alpha^TK
$$

$$
=\min(K\alpha-Y)+\lambda\alpha^T=0
$$

最后得出论文中的公式2：
$$
\alpha=(K+\lambda I)^{-1}y
$$

### 引入循环矩阵求解$\alpha$

作者介绍了密集采样的概念，实际上密集采样的样本就是利用单一的跟踪窗口生成它的循环矩阵形式，然后去训练分类器。给出了**最重要**的一个公式：
$$
C(u)v=v\circ u=F^{-1}(F^*(u)\bullet F(v))
$$
$v\circ u$ 表示两个序列的周期拓扑相关运算，实际上和MOSSE的相关计算是一样的。

然后作者给出了循环矩阵的一个重要性质，循环矩阵对应的核函数也是循环矩阵形式的，那么对于 $\alpha$ 的求解就十分容易了。
$$
\alpha=(C{k}+\lambda C(\delta))^{-1}y=(C(k+\lambda\delta))^{-1}y
$$
其中，$k_i$是由单一窗口密集采样后生成的核函数对应值
$$
k_i=k(x,P^ix), \,\forall i=0,……,n-1
$$

$$
\alpha=(C(F^{-1}(F(k)+\lambda E)))^{-1}y=C(F^{-1}(\frac{1}{F(k)+\lambda}))y
$$

利用最重要的公式进行推导得到：
$$
\alpha=F^{-1}(\frac{F(y)}{F(k)+\lambda})
$$

### 目标跟踪的响应过程

在初始帧上得到$\alpha$后，对新来的图像进行目标跟踪过程，就是在讲搜索窗口代入高维空间的回归函数中，得到：
$$
Y=<W,\varphi(Z)>=\sum{\alpha_i<\varphi(X_i),\varphi(Z)>}=K\alpha
$$
K核函数依旧是一个循环矩阵：
$$
Y=\alpha C(k)
$$
利用上面最重要的那个公式就可以得到：
$$
Y=F^{-1}(F(\alpha)\bullet F(k))
$$
核函数的计算有很多种方法，本文采取的是Gauss函数：
$$
k^{gauss}=exp(-\frac{1}{\delta^2}(||x||^2+||x^,||^2-2F^{-1}(F(x)\bullet F^*(x^,))))
$$

### 流程图

![image](https://cdn.jsdelivr.net/gh/Shanyaliux/comment@master/csk/image.42wt9t9cxy00.png)

## 参考代码

[Github](https://github.com/Shanyaliux/TrackerCF/blob/master/tracker/CSK.py)

::: details

```python
from utils import *


class CSK:
    def __init__(self, imgPath):
        self.imgPath = imgPath
        self.frameList = getImgList(self.imgPath)
        self.frameList.sort()

        # 根据论文参数
        self.padding = 1.0  # 目标周围的额外区域
        # spatial bandwidth (proportional to target)
        self.output_sigma_factor = 1 / float(16)
        self.sigma = 0.2  # 高斯核带宽
        self.lambda_value = 1e-2  # 正则化
        self.interpolation_factor = 0.075  # 适应的线性插值因子

    def cos_window(self, sz):
        """
        width, height = sz
        j = np.arange(0, width)
        i = np.arange(0, height)
        J, I = np.meshgrid(j, i)
        cos_window = np.sin(np.pi * J / width) * np.sin(np.pi * I / height)
        """

        # cos_window = np.hanning(int(sz[1]))[:, np.newaxis].dot(np.hanning(int(sz[0]))[np.newaxis, :])
        cos_window = np.outer(np.hanning(int(sz[1])), np.hanning(int(sz[0])))
        return cos_window

    def gaussian2d_labels(self, sz, sigma):
        w, h = sz
        xs, ys = np.meshgrid(np.arange(w), np.arange(h))
        center_x, center_y = w / 2, h / 2
        dist = ((xs - center_x) ** 2 + (ys - center_y) ** 2) / (sigma ** 2)
        labels = np.exp(-0.5 * dist)
        return labels

    def _dgk(self, x1, x2):
        c = np.fft.fftshift(self.ifft2(self.fft2(x1) * np.conj(self.fft2(x2))))
        d = np.dot(x1.flatten().conj(), x1.flatten()) + np.dot(x2.flatten().conj(), x2.flatten()) - 2 * c
        k = np.exp(-1 / self.sigma ** 2 * np.clip(d, a_min=0, a_max=None) / np.size(x1))
        return k

    def fft2(self, x):
        # return np.fft.fft(np.fft.fft(x, axis=1), axis=0).astype(np.complex64)
        return np.fft.fft2(x)

    def ifft2(self, x):
        # return np.fft.ifft(np.fft.ifft(x, axis=1), axis=0).astype(np.complex64)
        return np.fft.ifft2(x)

    def _training(self, x, y):
        k = self._dgk(x, x)
        alphaf = self.fft2(y) / (self.fft2(k) + self.lambda_value)
        return alphaf

    def _detection(self, alphaf, x, z):
        k = self._dgk(x, z)
        responses = np.real(self.ifft2(alphaf * self.fft2(k)))
        return responses

    def track(self):

        info = loadImgInfo(self.frameList[0], 'CSK')
        initImg, initGt, target_sz, center = info

        initImg = cv2.cvtColor(initImg, cv2.COLOR_BGR2GRAY)
        initImg = initImg.astype(np.float32)

        x, y, w, h = tuple(initGt)

        self.w, self.h = w, h
        self._window = getCosWindow((int(round(2 * w)), int(round(2 * h))))
        self.crop_size = (int(round(2 * w)), int(round(2 * h)))
        self.x = cv2.getRectSubPix(initImg, (int(round(2 * w)), int(round(2 * h))), center) / 255 - 0.5
        self.x = self.x * self._window
        s = np.sqrt(w * h) / 16
        self.y = self.gaussian2d_labels((int(round(2 * w)), int(round(2 * h))), s)
        self._init_response_center = np.unravel_index(np.argmax(self.y, axis=None), self.y.shape)
        self.alphaf = self._training(self.x, self.y)

        for idx in range(len(self.frameList)):
            img = cv2.imread(self.frameList[idx])
            current_frame = img
            if len(current_frame.shape) == 3:
                assert current_frame.shape[2] == 3
                current_frame = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)
            current_frame = current_frame.astype(np.float32)
            z = cv2.getRectSubPix(current_frame, (int(round(2 * self.w)), int(round(2 * self.h))),
                                  center) / 255 - 0.5
            z = z * self._window
            self.z = z
            responses = self._detection(self.alphaf, self.x, z)
            curr = np.unravel_index(np.argmax(responses, axis=None), responses.shape)
            dy = curr[0] - self._init_response_center[0]
            dx = curr[1] - self._init_response_center[1]
            x_c, y_c = center
            x_c -= dx
            y_c -= dy
            center = (x_c, y_c)
            new_x = cv2.getRectSubPix(current_frame, (2 * self.w, 2 * self.h), center) / 255 - 0.5
            new_x = new_x * self._window
            self.alphaf = self.interpolation_factor * self._training(new_x, self.y) + \
                          (1 - self.interpolation_factor) * self.alphaf
            self.x = self.interpolation_factor * new_x + (1 - self.interpolation_factor) * self.x
            rect = np.array([center[0] - self.w / 2, center[1] - self.h / 2, self.w, self.h]).astype(np.int64)
            # print(rect)
            cv2.rectangle(img, (rect[0], rect[1]), (rect[0] + rect[2], rect[1] + rect[3]), (255, 0, 0), 2)
            cv2.imshow('CSK', img)
            cv2.waitKey(20)


img_path = '../datasets/surfer'
tracker = CSK(img_path)
# tracker.track()
s = np.sqrt(200 * 150) / 16
y = tracker.gaussian2d_labels((int(round(2 * 200)), int(round(2 * 150))), s)

cv2.imshow("1", y)
cv2.waitKey(0)
```

:::

## 参考文章

[目标跟踪系列--CSK算法](https://zhuanlan.zhihu.com/p/63403319)

[ECCV2012跟踪算法CSK原理及代码解析](https://blog.csdn.net/qq_17783559/article/details/82321239)

[CSK跟踪算法简介及代码的解读](https://blog.csdn.net/u012762641/article/details/74213256)

